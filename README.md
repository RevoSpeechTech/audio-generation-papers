## Audio Generation Papers
Recent audio generation (and audio codec) papers, including speech, music and general audios.

| **Year** | **Org.** | **Name** | **Title** | **Paper** | **Demo** | **Code** |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 2020 | OpenAI | Jukebox | Jukebox: A Generative Model for Music | [[2005.00341]](https://arxiv.org/pdf/2005.00341.pdf) | [[demo]](https://openai.com/research/jukebox) | [[code]](https://github.com/openai/jukebox) |
| 2021 | Google | Soundstream | Soundstream: An end-to-end neural audio codec | [[2107.03312]](https://arxiv.org/pdf/2107.03312.pdf)| [[demo]](https://google-research.github.io/seanet/soundstream/examples/) | [[code]](https://github.com/google/lyra)<br>[[code]](https://github.com/wesbz/SoundStream) |
| 2021 | IRCAM | RAVE | RAVE: A variational autoencoder for fast and high-quality neural audio synthesis | [[2111.05011]](https://arxiv.org/pdf/2111.05011.pdf) | [[demo]](https://anonymous84654.github.io/RAVE_anonymous/) | [[code]](https://github.com/acids-ircam/RAVE) |
| 2022 | Google | Perceiver-AR | General-purpose, long-context autoregressive modeling with Perceiver AR | [[2202.07765]](https://arxiv.org/pdf/2202.07765.pdf) | [[demo]](https://storage.googleapis.com/perceiver-ar/index.html) | [[code]](https://github.com/google-research/perceiver-ar)<br>[[code]](https://github.com/asigalov61/Perceiver-Music-Transformer) |
| 2022 | Stanford | SASHIMI | It's raw! audio generation with state-space models | [[2202.09729]](https://arxiv.org/pdf/2202.09729.pdf) | [[demo]](https://hazyresearch.stanford.edu/sashimi-examples/) | [[code]](https://github.com/albertfgu/diffwave-sashimi) | 
| 2022 | Baidu | A3T | A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing | [[2203.09690]](https://arxiv.org/pdf/2203.09690.pdf) | [[demo]](https://github.com/richardbaihe/a3t) | [[code]](https://github.com/richardbaihe/a3t) |
| 2022 | SJTU | VQTTS | VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature | [[2204.00768]](https://arxiv.org/pdf/2204.00768.pdf) | [[demo]](https://cpdu.github.io/vqtts/) | [[code]](https://github.com/vliu15/speech-masters-thesis) |
| 2022 | Google | Spectrogram Diffusion | Multi-instrument Music Synthesis with Spectrogram Diffusion | [[2206.05408]](https://arxiv.org/pdf/2206.05408.pdf) | [[demo]](https://colab.research.google.com/github/magenta/music-spectrogram-diffusion/blob/main/music_spectrogram_diffusion/colab/synthesize_midi.ipynb) | - |
| 2022 | Microsoft | DelightfulTTS 2 | DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders | [[2207.04646]](https://arxiv.org/pdf/2207.04646.pdf) | [[demo]](https://cognitivespeech.github.io/delightfultts2)| - | 
| 2022 | Google | MuLan | Mulan: A joint embedding of music audio and natural language | [[2208.12415]](https://arxiv.org/pdf/2208.12415.pdf) | - | [[code]](https://github.com/lucidrains/musiclm-pytorch/blob/main/musiclm_pytorch/musiclm_pytorch.py) | 
| 2022 | Google | AudioLM | AudioLM: a Language Modeling Approach to Audio Generation | [[2209.03143]](https://arxiv.org/pdf/2209.03143.pdf) | [[demo]](https://google-research.github.io/seanet/audiolm/examples/) | [[code]](https://github.com/lucidrains/audiolm-pytorch) | 
| 2022 | Meta AI | AudioGen | AudioGen: Textually Guided Audio Generation | [[2209.15352]](https://arxiv.org/pdf/2209.15352.pdf) | [[demo]](https://felixkreuk.github.io/audiogen/) |  - | 
| 2022 | Microsoft | Museformer | Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation | [[2210.10349]](https://arxiv.org/pdf/2210.10349.pdf) | [[demo]](https://ai-muzic.github.io/museformer/) | [[code]](https://github.com/microsoft/muzic/tree/main/museformer) | 
| 2022 | Meta AI | Encodec | High Fidelity Neural Audio Compression | [[2210.13438]](https://arxiv.org/pdf/2210.13438.pdf) | [[demo]](https://ai.honu.io/papers/encodec/samples.html) | [[code]](https://github.com/facebookresearch/encodec) | 
| 2022 | Meta AI | Modified AudioGen | Audio Language Modeling using Perceptually-Guided Discrete Representations | [[2211.01223]](https://arxiv.org/pdf/2211.01223.pdf) | - | - | 
| 2022 | Baidu | ERNIE-SAT | ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech | [[2211.03545]](https://arxiv.org/pdf/2211.03545.pdf) | [[demo]](https://brainy-candy-0a2.notion.site/ERNIE-SAT-DEMO-6f3ef7fbea944d9db46ba2770ab6693d) | [[code]](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/aishell3_vctk/ernie_sat) | 
| 2023 | Microsoft | PromptTTS | PromptTTS: Controllable Text-to-Speech with Text Descriptions | [[2211.12171]](https://arxiv.org/pdf/2211.12171.pdf) | [[demo]](https://speechresearch.github.io/prompttts/) | - | 
| 2023 | Microsoft | VALL-E | Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers | [[2301.02111]](https://arxiv.org/pdf/2301.02111.pdf) | [[demo]](https://valle-demo.github.io/) | [[code]](https://github.com/enhuiz/vall-e) | 
| 2023 | - | Msanii | Msanii: High Fidelity Music Synthesis on a Shoestring Budget | [[2301.06468]](https://arxiv.org/pdf/2301.06468.pdf) | - | [[code]](https://github.com/Kinyugo/msanii) |
| 2023 | Google | MusicLM | MusicLM: Generating Music From Text | [[2301.11325]](https://arxiv.org/pdf/2301.11325.pdf) | [[demo]](https://google-research.github.io/seanet/musiclm/examples/) | [[code]](https://github.com/lucidrains/musiclm-pytorch) |
| 2023 | ETH | Moûsai | Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion | [[2301.11757]](https://arxiv.org/pdf/2301.11757.pdf) | [[demo]](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb) | [[code]](https://github.com/archinetai/audio-diffusion-pytorch) |
| 2023 | CVSSP | AudioLDM | AudioLDM: Text-to-Audio Generation with Latent Diffusion Models | [[2301.12503]](https://arxiv.org/pdf/2301.12503.pdf) | [[demo]](https://audioldm.github.io/) | [[code]](https://github.com/haoheliu/AudioLDM) |
| 2023 | ByteDance | Make-An-Audio | Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models | [[2301.12661]](https://arxiv.org/pdf/2301.12661.pdf) | [[demo]](https://text-to-audio.github.io/) | - |
| 2023 | Google | SingSong | SingSong: Generating musical accompaniments from singing | [[2301.12662]](https://arxiv.org/pdf/2301.12662.pdf) | [[demo]](https://storage.googleapis.com/sing-song/index.html) | - |
| 2023 | ETH | ArchiSound | ArchiSound: Audio Generation with Diffusion | [[2301.13267]](https://arxiv.org/pdf/2301.13267.pdf) | [[demo]](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb) | [[code]](https://github.com/archinetai/audio-diffusion-pytorch) |
| 2023 | Tencent | InstructTTS | InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt | [[2301.13662]](https://arxiv.org/pdf/2301.13662.pdf) | [[demo]](http://dongchaoyang.top/InstructTTS/) | - |
| 2023 | Sapienza University | MSDM | Multi-Source Diffusion Models for Simultaneous Music Generation and Separation | [[2302.02257]](https://arxiv.org/pdf/2302.02257.pdf) | [[demo]](https://gladia-research-group.github.io/multi-source-diffusion-models/) | [[code]](https://github.com/gladia-research-group/multi-source-diffusion-models) |
| 2023 | Google | SPEAR-TTS | Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision | [[2302.03540]](https://arxiv.org/pdf/2302.03540.pdf) | [[demo]](https://google-research.github.io/seanet/speartts/examples/) | [[code]](https://github.com/collabora/spear-tts-pytorch) |
| 2023 | Google | Noise2Music | Noise2Music: Text-conditioned Music Generation with Diffusion Models | [[2302.03917]](https://arxiv.org/pdf/2302.03917.pdf) | [[demo]](https://google-research.github.io/noise2music/) | - |
| 2023 | CMU | MQTTS | A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech | [[2302.04215]](https://arxiv.org/pdf/2302.04215.pdf) | [[demo]](https://github.com/b04901014/MQTTS) | [[code]](https://github.com/b04901014/MQTTS) |
| 2023 | Baidu | ERNIE-Music | ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models | [[2302.04456]](https://arxiv.org/pdf/2302.04456.pdf) | - | - |
| 2023 | Microsoft | FoundationTTS | FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model | [[2303.02939]](https://arxiv.org/pdf/2303.02939.pdf) | [[demo]](https://github.com/cognitivespeech/cognitivespeech.github.io/commit/19ace4c953d77e548ba60e06feeaf4b5a5ce9d2b) | - |
| 2023 | Microsoft | VALL-EX | Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling | [[2303.03926]](https://arxiv.org/pdf/2303.03926.pdf) | [[demo]](https://vallex-demo.github.io/) | - |